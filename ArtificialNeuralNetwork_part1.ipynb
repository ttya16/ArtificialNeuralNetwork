{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ArtificialNeuralNetwork_part1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttya16/ArtificialNeuralNetwork/blob/master/ArtificialNeuralNetwork_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpNBiGhbEdb3",
        "colab_type": "text"
      },
      "source": [
        "# 人工ニューラルネット\n",
        "\n",
        "・人工ニューラルネットの考えは古くからあり、初めて考案されたのは1943年\"A Logical Calculus of Ideas Immanent in Nervous Activity\"(W. McCulloch & W. Pitts)によるもの。\n",
        "\n",
        "・動物の脳の神経が共同作業で命題論理を駆使して複雑な計算を実行している仕組みをシンプルにしたモデル。\n",
        "\n",
        "・過去に2度注目される時代があったが、当時は期待された程の性能を発揮できなかったことや、他の機械学習モデルの方が優秀だったこともありなかなか陽の目を見ることがなかった。\n",
        "\n",
        "・現在ニューラルネットは3度目のブームを迎えているが、果たして長続きするのか？\n",
        "\n",
        "→以下の理由により今回は長続きしそうである\n",
        "\n",
        "\n",
        "1.   ニューラルネットの学習に必要なデータが膨大に存在する。\n",
        "2.   計算機の性能向上により、大規模なニューラルネットを合理的な時間内に学習させられるようになった。\n",
        "3.   学習アルゴリズムの改良もされ続けており、地味だが大きく貢献している。\n",
        "4.   ニューラルネットの理論限界は実務上はほぼ無害に等しいことが明らかになった。（局所最適解に陥りやすい問題とかあったが、実際にはそうなるのは稀であるし、その局所最適解も全体の最適解にかなり近いものであることがほとんどであるらしい）\n",
        "5.  ブームがきて、資金調達→設備投資などの環境改善→より性能の良い製品→更に関心を産み、新たに資金調達できる　といった良い循環に入ってきている。\n",
        "\n",
        "\n",
        "\n",
        "※McCulloch & Pittsの提案したモデルは、1個以上のバイナリ入力（0 or 1）と1個のバイナリ出力を持ち、論理演算AND、OR、NOTと恒等写像を組み合わせて様々な論理演算を計算できる。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2HcGkdGJLue",
        "colab_type": "text"
      },
      "source": [
        "## パーセプトロン\n",
        "・最も単純な人工ニューロンで、1957年フランク・ローゼンブラット氏によって考案された。\n",
        "\n",
        "・LTU(線形閾値素子)と呼ばれるものが構成素子となっている。\n",
        "\n",
        "・入力はバイナリではなく数値で、重みづけされた後にステップ関数を適用して結果を出力する。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTlT2-e_ED3v",
        "colab_type": "code",
        "outputId": "4d00d7c3-6eab-47c2-8202-7dad796be44c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, (2, 3)] #petal length & width\n",
        "y = (iris.target == 0).astype(np.int) #setona or not setona\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "per_clf = Perceptron(random_state=42)\n",
        "per_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = per_clf.predict(X_test)\n",
        "print(f1_score(y_test, y_pred))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(120, 2)\n",
            "1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NESIFvjH8-7",
        "colab_type": "code",
        "outputId": "ece43d14-335d-4dc0-8bb0-6d6b4bfec30b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "#kerasによる実装\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, kernel_initializer='random_uniform'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 2)                 6         \n",
            "=================================================================\n",
            "Total params: 6\n",
            "Trainable params: 6\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr7Y6e-SMEtW",
        "colab_type": "text"
      },
      "source": [
        "## MLP & BackPropagation\n",
        "### MLP\n",
        "・パーセプトロンは有能に思えたが、ごく単純な問題（例えばXORなど）を解決できないなどの問題もあった。\n",
        "\n",
        "・この問題は複数のパーセプトロンを重ねることによって対応できることが判明した。これを多層パーセプトロン（Multi-Layer Perceptron, MLP）と呼ぶ。\n",
        "\n",
        "・入力層と出力層の間の層は隠れ層と呼ばれる。複数の隠れ層をもつニューラルネットワークを、\n",
        "深層ニューラルネットワーク(Deep Neural Network, DNN)という。\n",
        "\n",
        "\n",
        "### BackPropagation\n",
        "・MLPの学習方法については長年模索が続いたが、1986年にD. RumelhartらによりBackPropagation(誤差逆伝播法)というアルゴリズムが考案された。\n",
        "\n",
        "・リバースモード自動微分を用いた勾配急下法と直感的に考えてもらって良い。\n",
        "\n",
        "・BackPropagatioinは何をしているか\n",
        "\n",
        "1.   学習データがネットワークに与えられて、各層におけるニューロンの出力を計算する。\n",
        "（前進パス、ForwardProp）\n",
        "2.   ネットワークの出力誤差を計算し、出力層から今度はネットワークを逆に辿りながら隠れ層のニューロンの出力結果が誤差にどの程度影響を与えたか(誤差勾配)を計算する。\n",
        "3.    入力層まで達したら、誤差を小さくするように2.で計算した誤差勾配に基づいて各層の重みを調整する。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZZimH1lQWUg",
        "colab_type": "text"
      },
      "source": [
        "## 活性化関数の導入\n",
        "・BackPropagationをうまく動かすために、今までステップ関数を適用していたのを別の活性化関数に置き換えた。\n",
        "\n",
        "・ステップ関数は微分不可能なので勾配の計算には向かない。\n",
        "\n",
        "・代わりに導入したのがシグモイド関数と呼ばれるもの。\n",
        "\n",
        "・他にも有効な活性化関数として双曲線正接（hyperbolic tangent, tanh）やReLUがある\n",
        "\n",
        "###なぜ活性化関数？\n",
        "・活性化関数なしのNNはどれだけ多層にしたとて、表現力は1層のNNと変わらない。\n",
        "\n",
        "・1層のネットワークは線形モデルなので、単純にそれを多層にしたところであまり意味はない。\n",
        "\n",
        "・活性化関数を通して、ネットワークに非線形性を与える。\n",
        "\n",
        "\n",
        "### 万能近似定理 Universal Approximation Theorem\n",
        "\n",
        "定数でない連続な有界単調増加な関数を隠れ層の出力に使った3層以上のニューラルネットワークは、任意の連続関数を任意の精度で表現可能である。\n",
        "\n",
        "\n",
        "簡単に言うと、非線形な隠れユニットを使用していれば3層のネットワークで大体の問題は解けるということ。\n",
        "\n",
        "※隠れ層のユニット数が指数的に大きくなるし、学習できる保障もない"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9H4M-3gLSz6",
        "colab_type": "code",
        "outputId": "247b59fa-6ea5-4869-da5f-740ae8ccaba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import numpy as np\n",
        "#sigmoid\n",
        "def sigmoid(x):\n",
        "  return 1./(1. + np.exp(-x))\n",
        "\n",
        "\n",
        "#tanh\n",
        "def tanh(x):\n",
        "  return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
        "#np.tanh(x)\n",
        "\n",
        "#ReLU\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "\n",
        "#softmax\n",
        "def softmax(x):\n",
        "  return np.exp(x)/np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "x = np.array([-0.5, 0.0, 0.5])\n",
        "\n",
        "print('sigmoid:',sigmoid(x))\n",
        "print('tanh:', tanh(x))\n",
        "print('ReLU:', relu(x))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigmoid: [0.37754067 0.5        0.62245933]\n",
            "tanh: [-0.46211716  0.          0.46211716]\n",
            "ReLU: [0.  0.  0.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKtcSEJJkEXz",
        "colab_type": "text"
      },
      "source": [
        "・MLPは個々の出力がバイナリになるため、分類問題に適していると言える。\n",
        "\n",
        "・複数クラスの中からの分類をするときは、出力層の活性化関数をsoftmax関数にすれば良い。\n",
        "\n",
        "・信号の流れが入力層→出力層の一方通行になっているが、\n",
        "\n",
        "このようなアーキテクチャのネットワークを**順伝播型ネットワーク（Feedforward neural network, FNN）**と呼ぶ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYI93YauaaYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#２層の順伝播型ネットワークの実装 from scratch\n",
        "import numpy as np\n",
        "\n",
        "#Sigmoid layer\n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.params=[]\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return sigmoid(x)\n",
        "\n",
        "#FC layer\n",
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.params = [W, b]\n",
        "    \n",
        "  def forward(self, x):\n",
        "    W, b = self.params\n",
        "    return np.dot(x, W) + b\n",
        "\n",
        "  \n",
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    I, H, O = input_size, hidden_size, output_size\n",
        "    \n",
        "    #initialize parameters(W and b)\n",
        "    W1 = np.random.randn(I, H)\n",
        "    b1 = np.random.randn(H)\n",
        "    W2 = np.random.randn(H, O)\n",
        "    b2 = np.random.randn(O)\n",
        "    \n",
        "    #create model layers in a list\n",
        "    self.layers =[\n",
        "        Affine(W1, b1),\n",
        "        Sigmoid(),\n",
        "        Affine(W2, b2)\n",
        "    ]\n",
        "    \n",
        "    #save parameters of every layer in a list\n",
        "    self.params=[]\n",
        "    for layer in self.layers:\n",
        "      self.params += layer.params\n",
        "      \n",
        "  def predict(self, X):\n",
        "    for layer in self.layers:\n",
        "      X = layer.forward(X)\n",
        "    return X\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gcW7o_jTbxa",
        "colab_type": "code",
        "outputId": "5166955d-8f7e-48aa-cdec-b346dc4d3c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x = np.random.randn(10, 2)\n",
        "model = TwoLayerNet(2, 4, 3)\n",
        "y = model.predict(x)\n",
        "print(x[0])\n",
        "print(y[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.90503698  0.96045752]\n",
            "[ 0.54018827 -0.18485194 -1.94234724]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "savYf-xAteJs",
        "colab_type": "code",
        "outputId": "92ac7873-73c0-49b9-9385-b4e29c5b374f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "#MNIST\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import np_utils\n",
        "\n",
        "np.random.seed(1671) #for reproductivity\n",
        "\n",
        "NB_EPOCH = 200\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 1\n",
        "NB_CLASSES = 10\n",
        "OPTIMIZER = SGD()\n",
        "N_HIDDEN = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "RESHAPED = 28 * 28\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wptho3ya2Xv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(60000, RESHAPED)\n",
        "X_test = X_test.reshape(10000, RESHAPED)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "#normalize\n",
        "X_train /= 255.\n",
        "X_test /= 255.\n",
        "\n",
        "#to_categorical\n",
        "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
        "y_test = np_utils.to_categorical(y_test, NB_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ej4lwwVbkSV",
        "colab_type": "code",
        "outputId": "5197f1be-0a2e-4651-ec6c-fc183ddd198a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 10)                7850      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AARrxqszdrhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compile時に損失関数、optimizerを指定\n",
        "model.compile(optimizer = 'sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHmGfkwd2sgs",
        "colab_type": "text"
      },
      "source": [
        "## ニューラルネットワークのハイパーパラメータ\n",
        "・NNはあらゆるネットワークトポロジー（ニューロンの接続方法）が利用でき、層の数やニューロンの数、活性化関数のタイプ、重みの初期化方法等、色々な設定項目がある。\n",
        "\n",
        "・NNは設定項目が多い＝柔軟性が高いのがメリットではあるが、逆にあらゆる設定項目を適切に調整するのは非常に難しいという点を考えると欠点でもある。\n",
        "\n",
        "・グリッドサーチと交差検証では対応に時間がかかりすぎるくらいハイパーパラメータの数が多い\n",
        "\n",
        "###隠れ層の数\n",
        "・深いネットワークは浅いネットワークよりもパラメータ効率が高い（要するに深いネットワークの方が少ないニューロンで複雑なモデルを表現できる）\n",
        "\n",
        "・ネットワークの下位層では低水準の構造（図形の一部分の線分など）、中間層では低水準構造を組み合わせた中間レベルの構造（円や四角形など）、上位層では中間レベルの構造を組み合わせた高水準な構造（顔など）をモデリングする　といったような階層構造になるため効率が良い\n",
        "\n",
        "・未知のデータに対する汎化性能もよくなる。すでに学習済みのネットワークの下位層を似たような別のモデル用に転移して再利用することが出来てしまうため、学習の手間が大幅に省ける。\n",
        "\n",
        "###隠れ層に含まれるニューロンの数\n",
        "・入力層と出力層のニューロンの数は扱うデータとターゲットによって予め決まっているが、隠れ層のニューロンについては自由に調整できる。普通は層ごとに数が減って漏斗のような形になる。\n",
        "\n",
        "・実際適切なニューロンの数を見つけるのは難しいので、Overfittingするまで適当にニューロン数を増やしてみるか、予め多めに設定しておいてDropoutを挟んで減らして調整する　といったような方法が考えられる。\n",
        "\n",
        "\n",
        "###活性化関数\n",
        "・隠れ層の活性化関数はReLUがほとんどである。（理由はなんとなくしかわかっていないが、他の活性化関数よりも計算にかかる時間が速い、かつ大きな値に大してもplateauにならない　といった利点によると考えられる）\n",
        "\n",
        "・出力層の活性化関数はバイナリクラス分類ならsigmoid、多クラス分類ならsoftmax、回帰なら活性化関数なし（恒等写像）　とするのが通常の方法である。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TatIb2c4YLLz",
        "colab_type": "text"
      },
      "source": [
        "# DNNの訓練\n",
        "非常に深いDNNを用いてより多くの特徴量をもつデータに対して多数のクラス分類タスクを実行しなければならない　といった話になってくると、以下のような問題に直面するだろう。\n",
        "\n",
        "\n",
        "*   下位層が非常に訓練しづらくなる**勾配消失**問題、または関連する**勾配爆発**問題\n",
        "*   大規模ネットワークの学習時間の長さ（非常に長い）\n",
        "*   パラメータの多さによる過学習の発生しやすさ\n",
        "\n",
        "これらの問題を解決するための様々なアプローチを挙げる。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os5GvHX7tfyB",
        "colab_type": "text"
      },
      "source": [
        "## 勾配消失/爆発　とは\n",
        "DNNではBackPropagationによってコスト関数の誤差勾配を出力層から入力層まで伝えて、パラメータの更新を行う。ところが多層のネットワークでは下位層に行けば行くほど勾配が緩やかになり、勾配急下法によるパラメータの更新が十分になされない（ほとんど変化しない）。よって最適解にたどり着かないまま更新が止まってしまうという状況になってしまうのだ。この問題を**勾配消失**という。\n",
        "\n",
        "またこれとは逆に、勾配が大きくなりすぎて各層のパラメータが更新されすぎてしまい、コスト関数の値が収束せずに発散してしまうことも起こりうる。この問題を**勾配爆発**という。\n",
        "\n",
        "この問題は経験的に観測されていたが、詳しいことがわかったのは2010年Xavier Glorotらによる\"Understanding the Difficulty of Training Deep Feedforward Neural Networks\"からで、シグモイド活性化関数や重みの初期化方法に問題があるという指摘がなされた。\n",
        "\n",
        "シグモイド関数は入力値の絶対値が大きいと1or-1で飽和して、誤差逆伝播の処理によって伝えられる勾配がほぼなくなってしまい、層が進むにつれてわずかな勾配が更に小さくなってしまい消えて行く。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFYNR8ZTxmXB",
        "colab_type": "text"
      },
      "source": [
        "## Xavierの初期値、Heの初期値\n",
        "・層に出入りする信号がなくなったり、爆発したり、飽和するのは困るので、それなら入出力の信号とその勾配の分散を等しくなるようにしてやれば良いのでは？という発想\n",
        "\n",
        "・Xavierの初期値ではシグモイド関数の場合は以下のように分散を設定して無作為に重みを初期化すると良いらしい。（詳しくは論文を読むべし）\n",
        "$$\n",
        "平均：0\\\\標準偏差：\\sigma = \\sqrt{\\frac{2}{n_{inputs} + n_{outputs}}}\\\\\n",
        "の正規分布\n",
        "$$\n",
        "または、\n",
        "\n",
        "$$\n",
        "r = \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}\\\\\n",
        "とした時の-r〜+rまでの一様分布\n",
        "$$\n",
        "\n",
        "\n",
        "・他の活性化関数に対してもそれぞれ初期値の設定方法が提案されており、中でもよく使うReLUに対する初期値はHeの初期値を用いると良い\n",
        "\n",
        "\n",
        "・kerasだとmodel.add(Dense(kernel_initializer='.....'))\n",
        "の部分で初期化法を設定することができるみたい。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjLvrDH55nlr",
        "colab_type": "text"
      },
      "source": [
        "## 活性化関数(ReLU, leakyReLU, ELU)\n",
        "・シグモイド関数に問題があるのであれば、他に良い活性化関数を見つけなければならない。\n",
        "\n",
        "・ReLUがはるかに優れていることがわかった。が、理論上は以下のような問題も残っている。\n",
        "*   ニューロンの入力加重総和が負の値になる時、ReLUは0を出力してしまい勾配も0なので、こうなると0しか出力しなくなってしまう（実質的にニューロンが死んでしまう）。\n",
        "\n",
        "・そのため改良版としてleaky ReLUなどの活性化関数も考案されている。\n",
        "$$\n",
        "LeakyReLU_\\alpha(z) = max(\\alpha z, z)\\\\\n",
        "\\alpha = 0.01　など\n",
        "$$\n",
        "\n",
        "\n",
        "・2015年にはELU(exponential linear unit)という活性化関数も考案され、ReLUよりも高い性能であることが示された。\n",
        "\n",
        "$$\n",
        "ELU_\\alpha(z) = \\begin{cases}\n",
        "\\alpha(exp(z) - 1) \\quad( z<0)\\\\\n",
        "z \\quad ( z\\geqq 0)\\\\\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXgMSwF3AKd9",
        "colab_type": "code",
        "outputId": "9428ef5d-e313-4842-c761-8253a84d8b96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "#ReLU, leakyReLU, ELU\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def ReLU(z):\n",
        "  return np.maximum(0, z)\n",
        "\n",
        "def leakyReLU(z, alpha=0.01):\n",
        "  return np.maximum(alpha*z, z)\n",
        "\n",
        "def ELU(z, alpha=1):\n",
        "  if z < 0:\n",
        "    return alpha*(np.exp(z) - 1)\n",
        "  else:\n",
        "    return z\n",
        "\n",
        "\n",
        "X = np.linspace(-1, 1, 100)\n",
        "y_relu = [ReLU(x) for x in X]\n",
        "y_leakyrelu = [leakyReLU(x, alpha=0.1) for x in X]\n",
        "y_elu = [ELU(x) for x in X]\n",
        "\n",
        "plt.plot(X, y_relu, color='k', label='ReLU')\n",
        "plt.plot(X, y_leakyrelu, color='r', linestyle='dashed', label='leakyReLU')\n",
        "plt.plot(X, y_elu, color='b', linestyle='dashdot', label='ELU')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFKCAYAAAAnj5dkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFkIIITRGmrMQQgihMf8A3uwwGQ+7LsMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsbAR8eFFHsX",
        "colab_type": "text"
      },
      "source": [
        "## BatchNormalization\n",
        "\n",
        "###mini batch学習\n",
        "学習データがN個あった時、N個全てを使って損失を計算して重みを更新する方法をBatch学習という。\n",
        "\n",
        "それに対して、N個のデータからランダムにn(<N)個選び、そのn個に対して損失を計算する方法をmini batch学習という。\n",
        "メリットとしては、データn個に対して損失を計算して重みを更新して行くので、その度に損失関数の形状が変わっていき、局所解に陥りにくくなる。\n",
        "また、1回の計算コストもBatch学習に比べて小さいので、計算も高速化させつつ正しい解へ辿り着きやすくなることが見込まれる。\n",
        "\n",
        "\n",
        "### モチベーション\n",
        "He, Xavierの初期値を導入すれば学習開始時には勾配消失/爆発の問題は解消されているが、学習過程ではわからない。\n",
        "2015年S.Ioffe, C.Szegedyらにより、勾配消失/爆発のみならず、学習過程で変化する各層のパラメータの影響により、各層への入力分布が変化していく問題（**内部共変量シフト**と呼ぶ）があると指摘され、その対策としてBatchNormalizationという方法が提案された。\n",
        "\n",
        "各層において活性化関数実行直前に、入力に対して0を中心としたセンタリングと正規化を行う。\n",
        "\n",
        "\n",
        "\n",
        "論文:[BatchNormalization](https://arxiv.org/pdf/1502.03167.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dtGM-viKKDE",
        "colab_type": "text"
      },
      "source": [
        "## 勾配クリッピング\n",
        "・勾配降下法において、更新時に大きすぎる変化があった場合はあえて更新幅を抑えるというテクニック。勾配爆発対策。\n",
        "\n",
        "・BatchNormalizationの方が人気だが、RNNなどではいまだに役に立つテクニックなので知っておいて損はないそう。\n",
        "\n",
        "\n",
        "論文:[On the difficulty of training recurrent neural networks  R.Pascanu et al.](http://proceedings.mlr.press/v28/pascanu13.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI99-oMMMf79",
        "colab_type": "text"
      },
      "source": [
        "## 転移学習(Transfer Learning)\n",
        "・大規模なDNNに取り掛かるにあたって、1からモデルを構築して学習させるのも良いが、果たしてそんな時間があるのか？\n",
        "\n",
        "・似たようなタスクをこなしているNNモデルがあれば、その下位層のネットワークを流用させてもらうということも可能で、そのほうが学習の手間が大幅に省けて良い。\n",
        "これを**転移学習**という。(実装はCNNのところでやる)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM82pmlSRnrm",
        "colab_type": "text"
      },
      "source": [
        "Part2へ続く\n",
        "DNNの訓練において、過学習を防ぐための方法や訓練の高速化のための手法を学ぶ\n",
        "・DataAugmentation\n",
        "・Optimizer(SGD, RMSProp, Adamなど)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSyRJZ_98CSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}